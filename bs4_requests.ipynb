{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01a8b87a-6c7d-4958-90f0-989a561e21a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.31.0\n",
      "4.12.2\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import bs4\n",
    "\n",
    "print(requests.__version__)\n",
    "print(bs4.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43502eca-0c03-405d-a525-3a9d16c53b18",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "URL = 'https://www.naver.com/'\n",
    "\n",
    "req = requests.get(URL)\n",
    "print(req.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d34fb9f-842c-49b8-ae73-6ce3fc93af55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# req.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "31c0e8b2-316d-4dfb-9f4a-30969cba7a32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<li>애플</li>, <li>삼성</li>, <li>노키아</li>, <li>LG</li>]\n",
      "['애플', '삼성', '노키아', 'LG']\n",
      "   회사명\n",
      "0   애플\n",
      "1   삼성\n",
      "2  노키아\n",
      "3   LG\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd \n",
    "with open('index.html', 'r', encoding='UTF8') as f:\n",
    "    \n",
    "    # step 01: 데이터 수집\n",
    "    contents = f.read()\n",
    "    \n",
    "    # step 02: 데이터 파싱 (순수한 HTML 파일을 BeautifulSoup 객체로 변환)\n",
    "    soup = BeautifulSoup(contents, 'lxml')\n",
    "    #print(soup)\n",
    "    \n",
    "    #print(soup.h2)\n",
    "    #print(soup.ul)\n",
    "    #print(\"-----\")\n",
    "    #print(soup.ul.li)\n",
    "    # 4개의 li 태그에 있는 회사명을 모두 가져오는 것이 목적\n",
    "    \n",
    "    # step 03: 데이터 수집을 위한 특정 태그 찾기\n",
    "    companies = []\n",
    "    print(soup.find_all('li'))\n",
    "    \n",
    "    # step 04: 데이터 가공\n",
    "    for tag in soup.find_all('li'):\n",
    "        companies.append(tag.text)\n",
    "    print(companies)\n",
    "    \n",
    "    # step 05: 처리된 데이터 저장 pandas 데이터프레임\n",
    "    crawling_dict = {'회사명': companies}\n",
    "    result = pd.DataFrame(crawling_dict)\n",
    "    print(result)\n",
    "    \n",
    "    # step 06: csv 파일로 내보내기 or DB로 내보내기\n",
    "    result.to_csv(\"result.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5ae997cf-0ec2-4473-8388-42b6d3c74f4b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                노래제목\n",
      "0      Love wins all\n",
      "1               Wife\n",
      "2              To. X\n",
      "3           Love 119\n",
      "4               에피소드\n",
      "..               ...\n",
      "95    When I Get Old\n",
      "96             밤, 바다\n",
      "97           사랑을 하다가\n",
      "98    안 아름답고도 안 아프구나\n",
      "99  그대가 있는 곳, 언제 어디든\n",
      "\n",
      "[100 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd \n",
    "\n",
    "def crawling(soup) :\n",
    "    # print(soup)\n",
    "    tbody = soup.find(\"tbody\")\n",
    "    result = []\n",
    "    for p in tbody.find_all('p', class_ = 'title'):\n",
    "        result.append(p.get_text().strip())\n",
    "    return result\n",
    "\n",
    "def main() :\n",
    "    custom_header = {\n",
    "        'referer' : 'https://music.bugs.co.kr/',\n",
    "        'user-agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
    "    }\n",
    "    url = \"https://music.bugs.co.kr/chart\" # 크롤링 하려는 웹사이트\n",
    "    req = requests.get(url, headers = custom_header)\n",
    "    \n",
    "    soup = BeautifulSoup(req.text, \"html.parser\")\n",
    "    crawling(soup)\n",
    "\n",
    "    titles = crawling(soup)\n",
    "    print(pd.DataFrame({\"노래제목\" : titles}))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\" :\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a043ad0-bb9a-42fc-a0b6-77b5941942bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             날짜       종가     전일비       시가       고가       저가         거래량\n",
      "0    2024.02.01  73800.0  1100.0  73000.0  73800.0  72900.0   5009174.0\n",
      "1    2024.01.31  72700.0  1600.0  73400.0  74000.0  72500.0  15703560.0\n",
      "2    2024.01.30  74300.0   100.0  75000.0  75300.0  73700.0  12244418.0\n",
      "3    2024.01.29  74400.0  1000.0  73800.0  75200.0  73500.0  13976521.0\n",
      "4    2024.01.26  73400.0   700.0  73700.0  74500.0  73300.0  11160062.0\n",
      "..          ...      ...     ...      ...      ...      ...         ...\n",
      "115  2023.08.11  67500.0   500.0  68400.0  68800.0  67500.0   9781038.0\n",
      "116  2023.08.10  68000.0   900.0  68300.0  68500.0  67800.0  10227311.0\n",
      "117  2023.08.09  68900.0  1300.0  68000.0  69600.0  67900.0  17259673.0\n",
      "118  2023.08.08  67600.0   900.0  69000.0  69100.0  67400.0  14664709.0\n",
      "119  2023.08.07  68500.0   200.0  67700.0  69200.0  67600.0  10968505.0\n",
      "\n",
      "[120 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import warnings\n",
    "import time\n",
    "import random\n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "def crawling(url, headers, soup):\n",
    "    last_page = int(soup.select_one('td.pgRR').a['href'].split('=')[-1])\n",
    "    \n",
    "    df = None\n",
    "    count = 0\n",
    "    for page in range(1, last_page + 1):\n",
    "      req = requests.get(f'{url}&page={page}', headers=headers)\n",
    "      df = pd.concat([df, pd.read_html(req.text, encoding = \"euc-kr\")[0]], ignore_index=True)\n",
    "      if count > 10:\n",
    "        break\n",
    "      count += 1\n",
    "      time.sleep( random.uniform(2,4)) \n",
    "\n",
    "    df.dropna(inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    \n",
    "    return df\n",
    "\n",
    "def main():\n",
    "    company_code = '005930' # 삼성전자\n",
    "    url =\"https://finance.naver.com/item/sise_day.nhn?code=\" + company_code\n",
    "    \n",
    "    headers = { \n",
    "             'referer' : 'https://finance.naver.com/item/sise.naver?code=005930',\n",
    "             'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
    "            }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    result = crawling(url, headers, soup)\n",
    "    print(result)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25fa666-7cea-41e1-b34d-83525e13bf99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
